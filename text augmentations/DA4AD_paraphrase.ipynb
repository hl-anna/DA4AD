{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DA4AD_paraphrase.ipynb","provenance":[{"file_id":"13Jn3BxfKNf9vKGhdkU7IrEgsGAEiCIy8","timestamp":1658169005489},{"file_id":"https://github.com/dataprofessor/parrot/blob/main/PARROT.ipynb","timestamp":1636920792847}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"b7PeNhaVM3sH"},"source":["#Initial Setup"]},{"cell_type":"code","metadata":{"id":"JeObOANOHSZ8"},"source":["%%capture\n","!pip install git+https://github.com/PrithivirajDamodaran/Parrot.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fIN60CHYn7Ub"},"source":["%%capture\n","!pip install transformers\n","!pip install SentencePiece"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"82aBk31THVG1"},"source":["# Import libraries\n","from parrot import Parrot\n","import torch\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","import transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M8jkvnL2HlY4"},"source":["# For reproducibility\n","SEED = 42\n","\n","device = 'cpu'\n","if torch.cuda.is_available():\n","    device = 'cuda'\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"38QohuW-HwG9"},"source":["#Init models (make sure you init ONLY once if you integrate this to your code)\n","parrot = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\", use_gpu=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RiGOJaoOFQrD"},"source":["# Load Data\n","\n"]},{"cell_type":"code","metadata":{"id":"dYekMCyoVUgf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647380415929,"user_tz":0,"elapsed":62339,"user":{"displayName":"Domi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB3C7bRk6rNLcUp8Ib99giowLBogh74wvz2nN-qg=s64","userId":"15731206484394126583"}},"outputId":"c0b59849-3dd6-4656-ddc6-b4e465f9c4c1"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","### TO CHANGE ######\n","DIR = f\"/content/gdrive/MyDrive/Path/to/Dementiabank/folder\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["# Load the ids of AD subjects into a list\n","with open(f'{DIR}/audio_filenames_dementia.txt', \"r\") as clf:\n","    lines = clf.readlines()\n","ids_ad = [re.sub('\\n', '', line) for line in lines]\n","\n","# Load the ids of Control subjects into a list\n","with open(f'{DIR}/audio_filenames_control.txt', \"r\") as clf:\n","    lines = clf.readlines()\n","ids_hc = [re.sub('\\n', '', line) for line in lines]\n","\n","path_ad = 'path_to_ad_data'\n","path_hc = 'path_to_hc_data'\n","test_path = 'path_to_test_data'\n","test_id_path = 'path_to_a_txt_file_with_test_sample_labels'\n","\n","data_ad, labels_ad, aug_dataset_ad = data_to_str(ids_ad, path_ad, AD_flag=1,\n","                                                 augment=False) \n","data_hc, labels_hc, aug_dataset_hc = data_to_str(ids_hc, path_hc, AD_flag=0,\n","                                                 augment=False)\n","dataset, labels, aug_dataset = [], [], []\n","\n","dataset.extend(data_ad)\n","dataset.extend(data_hc)\n"],"metadata":{"id":"H_0uJE0atrG7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lxZZkOVwNFxJ"},"source":["# Paraphrase"]},{"cell_type":"code","metadata":{"id":"f7GOSiDoPuAP"},"source":["import torch\n","from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n","\n","model_name = 'tuner007/pegasus_paraphrase'\n","torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","tokenizer = PegasusTokenizer.from_pretrained(model_name)\n","model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n","\n","def get_response(input_text,num_return_sequences,num_beams):\n","  tokenizer = PegasusTokenizer.from_pretrained(model_name)\n","  batch = tokenizer(input_text,truncation=True,padding='longest',max_length=30, return_tensors=\"pt\").to(torch_device)\n","  translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=0.5)\n","  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n","  return tgt_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7TZc4e3wPuxW"},"source":["def paraphrase(text):\n","  text = text.replace('[SEP]','')\n","  txt = text.split('.')\n","  txt = [x for x in txt if len(x.strip())>0]\n","  num_beams = 10\n","  num_return_sequences = 1\n","  print(len(txt))\n","\n","  start = 10\n","  end = min(20,len(txt))\n","  step = 10\n","  res = get_response(txt[:10],num_return_sequences,num_beams)\n","  \n","  while end!=len(txt):\n","    res.extend(get_response(txt[start:end],num_return_sequences,num_beams))\n","    start = end \n","    end = min(start+step,len(txt))\n","\n","  res_sent = ' '.join(res)\n","\n","  return res_sent\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PqB7INUQoWzX"},"source":["dataset['paraphrase'] = np.vectorize(paraphrase)(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inspect & Save"],"metadata":{"id":"mt8E75rXvqsw"}},{"cell_type":"code","metadata":{"id":"c6mx2ee_qzZw"},"source":["dataset.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nc3_5qgEcGvq"},"source":["dataset.to_csv(f'{DIR}/paraphrase_aug_dataset.csv',index=False)"],"execution_count":null,"outputs":[]}]}

{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DA4AD_text_synthesis.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPIoxKXpFAZR4aCYFjq+lJ/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pNuM8WxW1CMf"},"source":["# Setup\n"]},{"cell_type":"code","metadata":{"id":"8Tdng59Q1BU_"},"source":["%%capture\n"," !pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mlnpuswM0xHl","executionInfo":{"status":"ok","timestamp":1645141857259,"user_tz":0,"elapsed":12754,"user":{"displayName":"Domi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB3C7bRk6rNLcUp8Ib99giowLBogh74wvz2nN-qg=s64","userId":"15731206484394126583"}},"outputId":"d004d34c-4654-4594-de26-e216c22138aa"},"source":["import os\n","import time\n","import datetime\n","from google.colab import drive\n","\n","import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","import random\n","\n","import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n","torch.manual_seed(42)\n","\n","from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"Vm1xULZpL9SV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645141901869,"user_tz":0,"elapsed":44614,"user":{"displayName":"Domi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB3C7bRk6rNLcUp8Ib99giowLBogh74wvz2nN-qg=s64","userId":"15731206484394126583"}},"outputId":"6e7971b4-1a94-4616-bae1-a9b870b92273"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","### TO CHANGE ######\n","DIR = f\"/content/gdrive/MyDrive/Path/to/Dementiabank/folder\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# For reproducibility\n","SEED = 42\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)"],"metadata":{"id":"oEi0dCVBy0UW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DwQ4Hsh31KnG"},"source":["# Loading data"]},{"cell_type":"code","source":["# Load the ids of AD subjects into a list\n","with open(f'{DIR}/audio_filenames_dementia.txt', \"r\") as clf:\n","    lines = clf.readlines()\n","ids_ad = [re.sub('\\n', '', line) for line in lines]\n","\n","# Load the ids of Control subjects into a list\n","with open(f'{DIR}/audio_filenames_control.txt', \"r\") as clf:\n","    lines = clf.readlines()\n","ids_hc = [re.sub('\\n', '', line) for line in lines]\n","\n","path_ad = 'path_to_ad_data'\n","path_hc = 'path_to_hc_data'\n","test_path = 'path_to_test_data'\n","test_id_path = 'path_to_a_txt_file_with_test_sample_labels'\n","\n","data_ad, labels_ad, aug_dataset_ad = data_to_str(ids_ad, path_ad, AD_flag=1,\n","                                                 augment=False) \n","data_hc, labels_hc, aug_dataset_hc = data_to_str(ids_hc, path_hc, AD_flag=0,\n","                                                 augment=False)\n","\n","dataset, labels, aug_dataset = [], [], []\n","\n","dataset.extend(data_ad)\n","dataset.extend(data_hc)\n","\n","labels.extend(labels_ad)\n","labels.extend(labels_hc)\n"],"metadata":{"id":"Tkb7t-SBj48u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hFVP1yek1NKv"},"source":["# Finetuning GPT2\n"]},{"cell_type":"code","metadata":{"id":"h3jjhLyAHpCY"},"source":["class GPT2Dataset(Dataset):\n","  def __init__(self, txt_list,labels, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n","\n","    self.tokenizer = tokenizer\n","    self.input_ids = []\n","    self.attn_masks = []\n","\n","    for idx,txt in enumerate(txt_list):\n","      encodings_dict = tokenizer(f'<|{labels[idx]}|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\", return_attention_mask = True)\n","\n","      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n","      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n","    \n","  def __len__(self):\n","    return len(self.input_ids)\n","\n","  def __getitem__(self, idx):\n","    return self.input_ids[idx], self.attn_masks[idx] \n","\n","def format_time(elapsed):\n","    return str(datetime.timedelta(seconds=int(round((elapsed)))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ygG0Jp51bzj"},"source":["\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2', eos_token='<|endoftext|>', pad_token='<|pad|>') \n","train_dataset = GPT2Dataset(dataset,labels,tokenizer)\n","train_dataloader = DataLoader(\n","            train_dataset, \n","            sampler = RandomSampler(train_dataset),\n","            batch_size = batch_size,\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n","model = AutoModelWithLMHead.from_pretrained(\"gpt2\", output_hidden_states=False)\n","\n","model.resize_token_embeddings(len(tokenizer))\n","\n","model.to(device)\n"],"metadata":{"id":"_5LUw7989k13"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aznCmUjMKrbm"},"source":["## model params\n","epochs = 15\n","learning_rate = 5e-4\n","warmup_steps = 1e2\n","epsilon = 1e-8\n","\n","sample_step = 100\n","\n","optimizer = AdamW(model.parameters(),\n","                  lr = learning_rate,\n","                  eps = epsilon\n","                )\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = warmup_steps, \n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8eHvnG7EVUVW"},"source":["\n","## TRAINING\n","\n","for epoch_i in range(0, epochs):\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","\n","    total_train_loss = 0\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[0].to(device)\n","        b_masks = batch[1].to(device)\n","\n","        model.zero_grad()        \n","\n","        outputs = model(  b_input_ids,\n","                          labels=b_labels, \n","                          attention_mask = b_masks,\n","                          token_type_ids=None\n","                        )\n","\n","        loss = outputs[0]  \n","\n","        batch_loss = loss.item()\n","        total_train_loss += batch_loss\n","\n","        if step % sample_step == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n","            model.eval()\n","            sample_outputs = model.generate(\n","                                    bos_token_id=random.randint(0,2),\n","                                    do_sample=True,   \n","                                    top_k=50, \n","                                    max_length = 150,\n","                                    top_p=0.95, \n","                                    num_return_sequences=1\n","                                )\n","            for i, sample_output in enumerate(sample_outputs):\n","                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n","            \n","            model.train()\n","\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_train_loss / len(train_dataloader)       \n","  \n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","        \n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vi8JrEVpaw9D"},"source":["output_dir = './model_save/'\n","\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","model_to_save = model.module if hasattr(model, 'module') else model \n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","torch.save(args, os.path.join(output_dir, 'training_args.bin'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate"],"metadata":{"id":"DS7ujZqn2i1t"}},{"cell_type":"code","metadata":{"id":"Y8jomD54gjCH"},"source":["def gen(label):\n","  prompt = label\n","\n","  generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n","  generated = generated.to(device)\n","\n","  print(generated)\n","\n","  sample_outputs = model.generate(\n","                                  generated, \n","                                  bos_token_id=random.randint(1,30000),\n","                                  do_sample=True,   \n","                                  top_k=50, \n","                                  max_length = 150,\n","                                  top_p=0.75,\n","                                  num_return_sequences=220\n","                                  )\n","  return sample_outputs\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U3BDcWK8NgmR"},"source":["output_dir = './model_save/'\n","model = AutoModelWithLMHead.from_pretrained(output_dir,'')\n","tokenizer = GPT2Tokenizer.from_pretrained(output_dir,'/tokenizer_config.json')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g7gSiLmTa99h"},"source":["model.eval()\n","\n","rows = []\n","\n","lbs =['0','1']\n","\n","for l in lbs:\n","  sample_outputs = gen(f'<|{l}|>')\n","  for i, sample_output in enumerate(sample_outputs):\n","    decoded = tokenizer.decode(sample_output, skip_special_tokens=True)\n","    print(\"{}: {}\\n\\n\".format(i, decoded))\n","    rows.append([decoded.replace(f'<|{l}|>',''),l])\n","\n","df = pd.DataFrame(rows, columns = ['Text',\"Intent\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_csv(f'{DIR}/synth_aug_dataset.csv',index=False)"],"metadata":{"id":"GTSfrBaSP22n"},"execution_count":null,"outputs":[]}]}